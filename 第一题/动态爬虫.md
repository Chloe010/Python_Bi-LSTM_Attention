# 动态爬虫-交互

#### 爬虫基本思路：

```
1、首先确认爬取的网站是静态还是动态
    请求头：将python代码伪装成浏览器，对服务器发出请求，防止服务器识别出来爬虫脚本
    若爬取vip用户的网站需要加上cookie-登录信息。
# import urllib.request
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
    }
    data = bytes(urllib.parse.urlencode({"hello":"world"}),encoding="utf-8")
    url="https://www.5iai.com/#/jobList"
    req=urllib.request.Request(url=url,data=data,headers=headers,method="POST")
    r = urllib.request.urlopen(req)
    print(r.read().decode("utf-8"))

2、根据爬取的网站得到一些头部信息和body的JavaScript跳转链接得知：爬虫网站需要用到动态爬虫
3、根据网页检查network得到规律，爬取的两大网站规律如以下：
    pageNumber=n 表示浏览的页面为分页模块的第几页面
    pageSize=n   表示浏览的页面每一页有多少条数据
    &以后为访问网站的方法参数
    
    找工作：
    https://www.5iai.com/api/enterprise/job/public/es?pageSize=10&pageNumber=1&willNature=&function=&wageList=%255B%255D&workplace=&keyword=
    找人才：
    https://www.5iai.com/api/resume/baseInfo/public/es?pageSize=10&pageNumber=1&function=&skills=&workplace=&keyword=
    
4、将爬取到的动态网页存储为可视化json文件方便接下来对目标数据的获取和分析
5、使用正则表达式解析数据
6、保存数据

```

## 找工作.py

##### 爬虫应用包

```python
import json
import requests
import xlwt
```

#### 1、发送get请求

```python
url = "https://www.5iai.com/api/enterprise/job/public/es?pageSize=1580&pageNumber=1&willNature=&function=&wageList=%255B%255D&workplace=&keyword="
```

##### 爬取找工作网站的代码如下：

​	通过浏览可知找工作网站的有158个分页，每个分页有10条数据

​	为了方便将爬取的数据存储在单一的json文件，我将pageSize干脆设置为158*10=1580，这样就可以在一个页面显示所有数据。

​	请求头：

​		将python代码伪装成浏览器，对服务器发出请求，防止服务器识别出来爬虫脚本

​		若爬取vip用户的网站需要加上cookie-登录信息。



```python
headers = {
    "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
}
response = requests.get(url=url, headers=headers)
"""
    print(response)
    返回状态码200-请求成功。
"""
```

#### 2、获取数据 响应体的文本数据

```python
wordData = json.loads(response.text)
# print(name)  检查输出内容没有问题
```

#### 3、将请求到的数据存储为json文件方便查阅

```python
with open('work.json', 'w', encoding='utf-8') as f:
    jsonJ = json.dumps(response.json(), ensure_ascii=False)
    f.write(jsonJ)
```

##### 4、提取有效数据到列表中

##### 提取数据的基本思路：

​	1、通过浏览json文件得知：

​		work.json文件的数据布局：status-状态码；message-操作信息；data-爬取数据；以及一些页面数据和操作数据

​	2、到data下获取目标数据

​		我们需要的"工作id", "工作更新时间", "工作岗位名称", "工作岗位最低工资", "工作岗位最高工资", "工作岗位要求经验", "工作岗位要求学历", "工作岗位招收人数", "工作岗位具体地址","公司名称", "公司所属类型", "公司成立模式", "公司规模"等

​		数据大部分分布在data下的”content“目录下，直接通过列表索引或者切片索引即可获取。

​	3、将获取到的所有岗位数据根据id划分到每一个列表子目录下

​		其中：找工作网站有1574条岗位招聘数据，共获取13个字段特征

​					找人才网站有10908条人才投递数据，共获取9个字段特征

​	4、根据id排序划分到的总列表，将其保存到excel文件中方便后续解析数据

​		其中：”找工作.xls“共有1574行，13列数据

​					”找人才.xls“共有10908行，9列数据



##### 4、1 将所有获取到的文本数据存入wordData1列表

```python
wordData1 = []
for i in wordData['data']['content']:
    w_id = i['id']  # 工作id
    wordData1.append(w_id)
    updateTime = i['updateTime']  # 工作更新时间
    wordData1.append(updateTime)
    positionName = i['positionName']  # 工作岗位名称
    wordData1.append(positionName)
    minimumWage = i['minimumWage']  # 工作岗位最低工资
    wordData1.append(minimumWage)
    maximumWage = i['maximumWage']  # 工作岗位最高工资
    wordData1.append(maximumWage)
    exp = i['exp']  # 工作岗位要求经验
    wordData1.append(exp)
    educationalRequirements = i['educationalRequirements']  # 工作岗位要求学历 0-不限 2-大专 3-本科 4-硕士 5-博士
    wordData1.append(educationalRequirements)
    count = i['count']  # 工作岗位招收人数
    wordData1.append(count)
    enterpriseAddress = i['enterpriseAddress']['detailedAddress']  # 工作岗位具体地址
    wordData1.append(enterpriseAddress)
    shortName = i['enterpriseExtInfo']['shortName']  # 公司名称
    wordData1.append(shortName)
    industry = i['enterpriseExtInfo']['industry']  # 公司所属类型
    wordData1.append(industry)
    econKind = i['enterpriseExtInfo']['econKind']  # 公司成立模式
    wordData1.append(econKind)
    personScope = i['enterpriseExtInfo']['personScope']  # 公司规模
    wordData1.append(personScope)
# print(wordData1)
```



##### 4、2 将wordData1列表中的数据根据id划分为子列表

```python
col_list = []
for col in range(0, 1574):
    col_list.append(wordData1[13*col:13+13*col])
print(col_list)
```

#### 5、保存数据到excel中，文件名-找工作.xls  表名字-找工作

```python
book = xlwt.Workbook(encoding="utf-8", style_compression=0)
sheet = book.add_sheet('找工作', cell_overwrite_ok=True)
col = ("工作id", "工作更新时间", "工作岗位名称", "工作岗位最低工资", "工作岗位最高工资", "工作岗位要求经验", "工作岗位要求学历", "工作岗位招收人数", "工作岗位具体地址",
       "公司名称", "公司所属类型", "公司成立模式", "公司规模")
for i in range(0, 13):
    sheet.write(0, i, col[i])
for i in range(0, 1574):
    print("第%d条" % i)
    data = col_list[i]
    for j in range(0, 13):
        sheet.write(i + 1, j, data[j])
book.save('F:/找工作.xls')
```

###### **找人才与找工作的爬虫流程基本一致，只是数据量和字段名字有些不同**









































## 找工作_岗位技能.py

##### 爬虫应用包

```python
import json
import requests
import urllib.request
import xlwt
import os
import pandas as pd
```



#### 1、获取work.json

```python
# 定义文件路径
path = 'F:/c题爬虫/'

# 打开文件,r是读取,encoding是指定编码格式
# 首先获取爬取好的work.json文件，因为以下爬取的网站页面需要该文件中每个岗位的具体id
with open(path + 'work.json', 'r', encoding='utf-8') as fp:
    # print(type(fp))  # 输出结果是 <class '_io.TextIOWrapper'> 一个文件类对象
    # load()函数将fp(一个支持.read()的文件类对象，包含一个JSON文档)反序列化为一个Python对象
    data = json.load(fp)
    # print(type(data))  # 输出结果是 <class 'dict'> 一个python对象,json模块会根据文件类对象自动转为最符合的数据类型,所以这里是dict
fp.close()
```

#### 2、获取worj.json中的id属性

​	岗位详情的页面：

​		https://www.5iai.com/api/enterprise/job/public?id=1613439183576236032

​		“1613439183576236032”：这一串就是岗位id

​		直接获取岗位id去爬取岗位详情页面的岗位所需技能

```python
word_id = []
word_label = []
a = ['None']  # 给予空值列表，若不是列表而是字符串的话后期存储会出现问题
for i in data['data']['content']:
    w_id = i['id']  # 工作id
    word_id.append(w_id)
```

#### 3、拼接字符串url-访问链接去爬取岗位详情页面的数据

​	由于在爬取岗位详情页面的过程中发现在第91个页面后出现大量的完全空页

​	也就是说页面并没有出现任何信息，但不能使用空值判断

​	则通过代码观察min(os.path.getsize(path))发现完全空页的文件大小在44-48字节之中

​	那么直接对文件大小进行判断

​	文件大于50字节且拥有关键的目标值才对其进行处理

​	否则均给空值

```python
for j in range(len(word_id)):
    id_url = "https://www.5iai.com/api/enterprise/job/public?id=" + str(word_id[j])
    response = requests.get(url=id_url)  # 获取页面
    path = str(j) + '.json'  # 拼接存储json文件的路径

    # 将获取到的岗位详情信息存储到json文件中方便读取
    with open(path, 'w', encoding='utf-8') as f:
        jsonJ = json.dumps(response.json(), ensure_ascii=False)
        f.write(jsonJ)
        # 获取json文件数据
        wordLabel = json.loads(response.text)

        # 获取目标数据 - 岗位要求具备的技能
        '''
            由于在爬取岗位详情页面的过程中发现在第91个页面后出现大量的完全空页
            也就是说页面并没有出现任何信息，但不能使用空值判断
            则通过代码观察min(os.path.getsize(path))发现完全空页的文件大小在44-48字节之中
            那么直接对文件大小进行判断
            文件大于50字节且拥有关键的目标值才对其进行处理
            否则均给空值
            
        '''
        aaa = os.path.getsize(path)
        if aaa > 50:
            if wordLabel['data']['skillsList']:
                label_name = wordLabel['data']['skillsList']
                if label_name:
                    aa = []
                    for i in label_name:
                        aa.append(i['labelName'])
                    word_label.append(aa)
                else:
                    word_label.append(a)
            else:
                word_label.append(a)
        else:
            word_label.append(a)
print(word_label)
```

#### 4、保存数据

```python
book = xlwt.Workbook(encoding="utf-8", style_compression=0)
sheet = book.add_sheet('岗位技能', cell_overwrite_ok=True)
col = "岗位技能"
for i in range(0, 1):
    sheet.write(0, i, col[i])
for i in range(0, len(word_id)):
    print("第%d条" % i)
    data = word_label[i]
    for j in range(0, 1):
        sheet.write(i + 1, j, data[j])
book.save('F:/找工作_岗位技能.csv')
```

#### 5、合并数据

```python
gangwei = pd.read_csv('F:/c题爬虫/找工作_岗位技能.csv')
work = pd.read_csv('F:/c题爬虫/找工作.csv')
df = pd.concat([gangwei, work], axis=0)  # 按行合并两个文件按数据
df.to_csv('F:/c题爬虫/找工作.csv')
```





































## 找人才.py

##### 爬虫应用包

```python
import json
import requests
import numpy as np
import pandas as pd
import xlwt
```

#### 1、爬取找人才网站的相关页面--准备网址路径

```python
url = "https://www.5iai.com/api/resume/baseInfo/public/es?pageSize=10910&pageNumber=1&function=&skills=&workplace=&keyword="
headers = {
    "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
}
```

#### 2、获取链接

```python
response = requests.get(url=url, headers=headers)
personData = json.loads(response.text)
```

#### 3、将爬取到的数据保存到json文件中

```python
with open('person.json', 'w', encoding='utf-8') as f:
    jsonJ = json.dumps(response.json(), ensure_ascii=False)
    f.write(jsonJ)
```

​	查看json文件可知：

​			"totalElements": "10908"-“找人才”网站总共10908条应聘者数据

​			"totalPages": 1091-总共爬取网页分页1091页

#### 4、获取json文件中的目标数据并保存到列表中

```python
personData1 = []
for i in personData['data']['content']:
    p_id = i['id']  # 人才id
    personData1.append(p_id)
    username = i['username']  # 姓名
    personData1.append(username)
    gender = i['gender']  # 性别 0-男 1-女
    personData1.append(gender)
    exp = i['exp']  # 工作经验
    personData1.append(exp)
    expectPosition = i['expectPosition']  # 应聘工作岗位
    personData1.append(expectPosition)
    willSalaryStart = i['willSalaryStart']  # 最低薪资期望
    personData1.append(willSalaryStart)
    willSalaryEnd = i['willSalaryEnd']  # 最高薪资期望
    personData1.append(willSalaryEnd)
    city = i['city']  # 所在城市
    personData1.append(city)
    updateTime = i['updateTime']  # 上传时间
    personData1.append(updateTime)
```

###### 特殊字段处理

​		关于“个人技能”字段的爬取，由于不是每一位应聘者都会拥有，而且拥有的技能数量也不对等，所以以下做了很多判断：

​				根据代码统计len(labelName['labelName'].max()),

​				应聘者拥有技能数额最多是8个，最少是0个。

​				需要根据“labelName”字段的长度去做不同的存储操作，防止存储过程中多存或者漏存的现象，保证数据的准确性。

```python
    labelName = i['skillMedalList']  # 个人技能
    if labelName:
        if len(labelName) == 0:
            personData1.append('None')
        elif len(labelName) == 1:
            a = labelName[0]['labelName']
            skillMedalList = [a]
            personData1.append(skillMedalList)
        elif len(labelName) == 2:
            a = labelName[0]['labelName']
            b = labelName[1]['labelName']
            skillMedalList = [a, b]
            personData1.append(skillMedalList)
        elif len(labelName) == 3:
            a = labelName[0]['labelName']
            b = labelName[1]['labelName']
            c = labelName[2]['labelName']
            skillMedalList = [a, b, c]
            personData1.append(skillMedalList)
        elif len(labelName) == 4:
            a = labelName[0]['labelName']
            b = labelName[1]['labelName']
            c = labelName[2]['labelName']
            d = labelName[3]['labelName']
            skillMedalList = [a, b, c, d]
            personData1.append(skillMedalList)
        elif len(labelName) == 5:
            a = labelName[0]['labelName']
            b = labelName[1]['labelName']
            c = labelName[2]['labelName']
            d = labelName[3]['labelName']
            e = labelName[4]['labelName']
            skillMedalList = [a, b, c, d, e]
            personData1.append(skillMedalList)
        elif len(labelName) == 6:
            a = labelName[0]['labelName']
            b = labelName[1]['labelName']
            c = labelName[2]['labelName']
            d = labelName[3]['labelName']
            e = labelName[4]['labelName']
            f = labelName[5]['labelName']
            skillMedalList = [a, b, c, d, e, f]
            personData1.append(skillMedalList)
        elif len(labelName) == 7:
            a = labelName[0]['labelName']
            b = labelName[1]['labelName']
            c = labelName[2]['labelName']
            d = labelName[3]['labelName']
            e = labelName[4]['labelName']
            f = labelName[5]['labelName']
            g = labelName[6]['labelName']
            skillMedalList = [a, b, c, d, e, f, g]
            personData1.append(skillMedalList)
        elif len(labelName) == 9:
            a = labelName[0]['labelName']
            b = labelName[1]['labelName']
            c = labelName[2]['labelName']
            d = labelName[3]['labelName']
            e = labelName[4]['labelName']
            f = labelName[5]['labelName']
            g = labelName[6]['labelName']
            h = labelName[7]['labelName']
            skillMedalList = [a, b, c, d, e, f, g, h]
            personData1.append(skillMedalList)
        elif len(labelName) == 10:
            a = labelName[0]['labelName']
            b = labelName[1]['labelName']
            c = labelName[2]['labelName']
            d = labelName[3]['labelName']
            e = labelName[4]['labelName']
            f = labelName[5]['labelName']
            g = labelName[6]['labelName']
            h = labelName[7]['labelName']
            i = labelName[8]['labelName']
            j = labelName[9]['labelName']
            skillMedalList = [a, b, c, d, e, f, g, h, i, j]
            personData1.append(skillMedalList)
    else:
        personData1.append('None')
```

#### 5、特征拆分

​		由于数据在列表中虽然是顺序，但并没有如字典那般对应到每一个应聘者，

​		则按照每一个应聘者均拥有10个特征字段的特性

​		将每10个特征分别拆开存储到另外一个列表中，也就是对应到每个应聘者。

```python
col_list = []
for col in range(0, 10908):
    col_list.append(personData1[10 * col:10 + 10 * col])
# print(col_list)  # 检查输出结果无误
```

#### 6、保存到文件中

```python
book = xlwt.Workbook(encoding="utf-8", style_compression=0)
# 设置表名
sheet = book.add_sheet('找人才', cell_overwrite_ok=True)
# 设置列名
col = ("人才id", "姓名", "性别", "工作经验", "应聘工作岗位", "最低薪资期望", "最高薪资期望", "所在城市", "上传时间", "个人技能")
# 将列名先列式存储到文件中
for i in range(0, 10):
    sheet.write(0, i, col[i])

# 对目标数据进行存储
for i in range(0, 10908):
    print("第%d条" % i)
    data = col_list[i]
    for j in range(0, 10):
        sheet.write(i + 1, j, data[j])
# 文件保存
book.save('F:/找人才.csv')
```