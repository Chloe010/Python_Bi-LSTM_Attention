# Python_Bi-LSTM_Attention
## 基于深度学习Python_Bi-LSTM_Attention的招聘与求职双向推荐模型
### 一、摘要
###### 近年来，python 作为一门编程语言，提供了高速的高级数据结构，以及简单有效地面向对象编程，随者人工智能和大数据的发展，python 逐渐成为流行 的编程语言；在新时代背景下，大学生毕业人数不断增加，大学生求职问题已成为广泛关注的社会热点。而且受疫情影响，诸多企业的招聘都改为线上进行，脱离时间和空间的限制，招聘需求不断上涨，从而出现就业竞争压力大、招聘与求职信息不对称等现象，双向职业推荐模型连接了企业与求职者的需求，利用 python语言对竞双向职业推荐模型的建立具有重大意义。
###### 通过从泰迪内推平台（https://www.5iai.com/#/index）的“找工作”页面和“找人才”页面，爬取所有招聘与求职信息并整理并形成文件：result1-1.csv以及 result1-2.csv，数据的爬取主要采用了 python 的爬虫技术。
###### 为了能准确的形成双向推荐模型我们们结合了 BI-LSTM 算法、文本匹配、Attention-LSTM 模型进行实现，基于 python 爬取得信息数据，我们对数据得空缺值处理以及异常值进行了处理，最后我们使用里 python 的 matplotlib 库、seaborn 库、对数据信息进行多个维度的用户画像，对数据之间的关系进行了数据可视化分析。基于预处理后的数据，构建岗位匹配度和求职者满意度的模型，使用 BI-LSTM 算法、文本匹配等算法，计算两表中的岗位匹配度以及满意度，每条招聘信息提供岗位匹配度非 0 的求职以及每位求职者提供求职者满意度非 0的招聘信息存放在指定的表格中。

### 二、章节安排
###### 根据前面相关内容的分析，我们针对用户企业双向推荐模型的研究思路主要从几节相应介绍：
###### 1） 第 3 主要介绍我们对泰迪内推网站的数据爬取过程
###### 2） 第 4 主要是介绍我们第二题的求职者以及企业多个维度的用户画像，使用matplotlib 库、seaborn 库，进行图像绘画。
###### 3） 第 5 主要介绍我们第三题的构建岗位匹配度和求职者满意度的模型，使用BI-LSTM 算法、文本匹配、Attention-LSTM 模型等

### 三、相关技术
##### 1、动态爬虫
###### 动态爬取是指爬取动态网页内容，包括通过 JavaScript 加载的内容。实现动态爬取的方法有以下几种：
###### 1. 使用 Selenium：Selenium 是一个自动化测试工具，可以模拟用户在浏览器中的操作，包括点击、输入、滚动等。通过 Selenium 可以启动一个浏览器，加载网页并执行 JavaScript，然后获取网页内容。Selenium 支持多种浏览器，包括 Chrome、Firefox、Safari 等。
###### 2. 使用 Pyppeteer：Pyppeteer 是一个 Python 库，可以通过调用 Chrome DevTools 协议来控制 Chrome 浏览器。Pyppeteer 可以启动一个 Chrome 浏览器，加载网页并执行 JavaScript，然后获取网页内容。Pyppeteer 的性能比 Selenium 更好，但需要安装 Chrome 浏览器。
###### 3. 使用 Requests-HTML：Requests-HTML 是一个 Python 库，可以发送 HTTP 请求并解析 HTML 内容。Requests-HTML 支持 JavaScript 渲染，可以通过调用浏览器引擎来执行 JavaScript，并获取渲染后的 HTML 内容。Requests-HTML 的性能比 Selenium 和 Pyppeteer 更好，但不支持所有的 JavaScript 特性。
###### 4. 使用 Splash：Splash 是一个轻量级的 JavaScript 渲染服务，可以通过 HTTP API 调用。Splash 可以启动一个浏览器，加载网页并执行 JavaScript，然后返回渲染后的 HTML 内容。Splash 支持多种浏览

##### 2、Numpy
###### NumPy（Numerical Python）是Python的一种开源的数值计算扩展。这种工具可用来存储和处理大型矩阵，比Python自身的嵌套列表（nested list structure)结构要高效的多（该结构也可以用来表示矩阵（matrix）），支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。

##### 3、Pandas
###### pandas 是基于NumPy 的一种工具，该工具是为解决数据分析任务而创建的。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。pandas提供了大量能使我们快速便捷地处理数据的函数和方法。

##### 4、Pyecharts
######  Pyecharts是一款将python与echarts结合的强大的数据可视化工具。使用 pyecharts 可以生成独立的网页,也可以在 flask , Django 中集成使用。

##### 5、Matplotlib 库
###### Python 开发中是专门用于开发 2D 图表(包括 3D 图表) 以渐进、交互式方式实现数据可视化，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形，可视化是在整个数据挖掘的关键辅助工具，可以清晰的理解数据，从而调整我们的分析方法。 能将数据进行可视化,更直观的呈现 使数据更加客观、更具说服力。

##### 6、BI-LSTM 算法
###### 前向的 LSTM 与后向的 LSTM 结合成 BiLSTM[1]，结构如下图 1 所示。前向 LSTM可以获取输入序列的过去数据信息，后向 LSTM 可以获取输入序列的过去数据信息，对时间序列实现向前和向后两次 LSTM 训练，进一步提高特征提取的全局性和完整性。
###### LSTM 的全称是 Long Short-Term Memory,它是 RNN 的一种。LSTM 模型是由 t时刻的输入词 Xt，细胞状态 Ct，临时细胞状态 Ct，隐层状态 ht，遗忘门 ft ，记忆门 it，输出门 ot 组成。LSTM 的计算过程可以概括为，通过对细胞状态中信息遗忘和记忆新的信息使得对后续时刻计算有用的信息得以传递，而无用的信息被丢弃，并在每个时间步都会输出隐层状态 ht，其中遗忘、记忆与输出由通过上个时刻的隐层状态 ht−1 和 X t 当前输入计算出来的遗忘门 ft，记忆门 it，输出门 ot 来控制。总体框架如下图 2-2：

##### 7、文本匹配
###### 文本匹配旨在从两端文本中挖掘内在的语义特诊，预测文本间相关行或者矛盾性。文本匹配任务是自然语言处理中重要的研究方向，无论是在信息检索、问题回答还是复述识别等任务中都扮演着重要角色。传统的文本匹配方法依赖于预定义的模板和人工提取的规则。随着深度学习的发展，深度神经网络已经普遍应用于自然语言处理任务中，以节省人工提取特征所耗费的成本和时间。文本匹配任务旨在给定两段文本Ｑ和Ｄ，通过提取文本中存在的语义信息和相似度特征来给出两段文本的相似度值，由最终的相似度值可以得知两段文本的内容是否属于相似的描述。基于深度学习的文本匹配模型大致可以分为两类：基于表示的文本匹配模型和基于交互的文本匹配模型。基于表示的文本匹配模型通过对文本进行预处理以及构建索引，能够更好地提取每段文本里面的信息，但是在信息表示的过程中容易失去语义焦点，难以衡量词在上下文中的重要性。基于交互的文本匹配模型虽然可以较好地把握语义焦点，针对上下文进行更好的建模，但是却忽视了全局信息，会造成无法刻画出全局匹配信息的后果[2]

##### 8、Attention-LSTM 模型
###### 将 LSTM 层的输出向量作为 Attention 层的输入。注意力机制的本质为计算某一特征向量的加权求和。 本文采用的是乘法注意力机制中的 ScaledDot-Product Attention 方法，其计算主要分为 3 个步骤：
###### 1. 将 query 和每个 key 进行点积计算得到权重
###### 2. 使用 Softmax 函数对权重归一化处理。
###### 3. 将权重和对应的 value 加权求和获得 Attention 。

##### 9、tensorflow 库
###### TensorFlow 是一个基于数据流编程（dataflow programming）的符号数学系统，被广泛应用于各类机器学习（machine learning）算法的编程实现，其前身是谷歌的神经网络算法库 DistBelief。Tensorflow 拥有多层级结构，可部署于各类服务器、PC 终端和网页并支持 GPU 和 TPU 高性能数值计算，被广泛应用于谷歌内部的产品开发和各领域的科学研究 。

##### 10、Tableau 可视化软件
###### Tableau 是能够帮助大家查看并理解数据的商业智能软件。 快速分析 在数分钟内完成数据连接和可视化。Tableau 比现有解决方案快 10 到 100 倍。 简单易用，任何人都可以使用。

### 四、招聘信息数据源的确定
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/012a5037-56f3-452a-8a9a-bb74138ee517)

### 五、网络爬虫的基本流程
#### 1、招聘信息数据源的确定
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/61003958-1d67-4037-909e-dc6851f4e1ef)

![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/79b7048f-9deb-4c45-8ac5-030d6d87ba1f)
###### 平台页面分析主要分为页面结构分析和内容分析两部分。页面结构分析主要包括以下方面：爬取目标页面是静态还是动态的；确定请求方式。内容分析是确定需要采集的内容在页面的什么位置，主要有两种情况：一是待采集的内容就在当前页面，可以直接爬取；二是待采集的内容需要提前制定采集规则[3]，利用当前页面进行跳转获取。信息爬取分为两步，发送请求和获取响应目标内容。利用Python 模拟浏览器对网页发出请求时主要使用 requests 库，请求方式为 get。返回请求成功时通过 json 形式获取响应的文本数据。数据解析是对获取到的数据进行内容判断和字段提取，选取具有分析价值的字段，由于平台数据格式较为规整，最后将解析后的数据保存到 csv 文件中，分别为 result1-1.csv 和result1-2.csv。

#### 2、数据类招聘信息的采集
##### 2.1 平台页面分析
###### 本文使用爬虫程序的目的是采集内推网站的岗位招聘信息和求职信息。在编写爬虫序之前需要根据采集内容确定网页位置，并从页面结构和内容两方面对其进行分析。
###### （1）网页结构分析
###### 网页结构分为以下部分：爬取目标页面是静态还是动态的，确定请求方式，是否存在反爬措施。判断爬取目标页面是静态还是动态的可以通过爬取部分头部信息，根据爬取的 body 和 JavaScript 跳转得出需要使用动态爬虫；通过 F12观察 network 请求列表得到爬取两大网站的规律：pageNumber 表示浏览的页面为分页模块的第几页面，PageSize 表示当前浏览页面的数据量，点击请求链接和响应标头部分发现该链接的请求方式是 get。
###### （2）内容分析
###### 泰迪内推“找工作”页面筛选条件主要有职位和企业两部分，通过观察职位页面的招聘数据得出每一条招聘信息包含工作岗位名称、薪资范围、学历要求、工作经验、招收人数、公司名称、公司类型、公司规模、发布时间和职位关键词等 10 类信息。职位页面每一页最多显示 10 条招聘信息，共 158 页数据，累计1574 条招聘信息。企业页面主要是企业的基本信息，包含企业名称、职位关键词、企业规模和招收人数 4 类信息，每一页最多显示 16 家企业信息，共 36 页数据，累计 568 条招聘信息。本次需要采集的数据为内推网站的招聘信息，通过对比分析发现求职页面信息包含企业页面信息，经过初步分析后决定爬取求职页面中的招聘信息。
##### 2.2 信息爬取
###### 通过对内推网站的页面结构和内容分析，将进一步爬取目标信息，使用requests 库的 get 方法向浏览器发出请求，其使用 requests 方法的语法格式如下：response=request.get(‘目标网址’)。请求成功后获取响应的文本数据，将请求到的数据存储为 json 格式文件，爬取岗位详情页面过程中发现第 91 个页面后出现大量的完全空页，使用 Python 代码观察 min(os.path.getsize(path))发现完全空页文件大小在 44-48 字节之中，直接对文件大小进行判断：文件大于50 字节且拥有关键的目标值才对其进行处理，否则均给空值。
##### 2.3 数据解析
###### 浏览 json 文件下的 data 目标数据发现有我们需要进一步分析的数据"工作id", "工作更新时间", "工作岗位名称", "工作岗位最低工资", "工作岗位最高工资", "工作岗位要求经验", "工作岗位要求学历", "工作岗位招收人数", " 工作岗位具体地址","公司名称", "公司所属类型", "公司成立模式", "公司规模"等，由于大部分数据在 content 目录下，直接通过列表索引或切片索引进行目标数据的获取。将获取到的所有岗位数据根据 id 划分到每一个列表子目录下，依据 id 排序划分到总列表，将其分别保存至 csv 文件中，文件分别为“result1-1.csv”和“result1-2.csv”。招聘信息保留招聘信息 ID、公司名称、招聘岗位、工作岗位最低工资、工作岗位最高工资、工作岗位要求经验、工作岗位要求学历、工作岗位招收人数、工作岗位具体地址、公司所属类型、公司成立模式、公司规模和岗位技能等 13 个字段特征；求职信息保留求职者 ID、姓名、性别、应聘工作岗位、工作经验、最低薪资期望、最高薪资期望、所在城市、上传时间和个人技能等 10 个字段特征来做进一步分析。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/01c33fbc-b2c3-412e-95fa-56819d695adf)

#### 3、招聘与求职信息分析
##### 3.1 数据预处理
###### 从泰迪内推平台的“找工作”页面和“找人才”页面，爬取的数据，较为规整，但为了能让计算机简单有效地处理文本信息，需要对原始的数据变得有规则、有结构、有组织的数据，需要进行数据预处理。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/ad64d478-b426-4906-af94-61c286787d0d)
###### 泰迪内推网站中“找工作”页面中，每条职位招聘信息排版样式如上图所示，顶部为岗位名称、薪资范围、学历要求、岗位要求、招聘人数、岗位发布时间、投递截止时间等基本信息；左下侧为职位关键词、技能要求、职位描述等具体信息，右侧为公司的基本信息，包括所属行业、公司名称、公司类型等。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/c07daa94-fb0b-4694-99c7-58cbc5aeb5a7)
###### 泰迪内推网站中“找人才”页面中，每条求职信息排版样式如上图所示，顶部为期望岗位名称、期望薪资；左侧求职者的基本信息，包括姓名、性别、年龄、工作经验、政治面貌、求职意向、简历关键词、工作经历、项目经历、竞赛经历等具体求职信息。
##### 3.1.1 数据清洗
###### 对所得的数据进行缺失值的处理，减少缺失值对画像的不准确性，在分析求职者以及企业招聘数据时，使用 python 的 isnull 对数据的每一列进行计算缺失值，相关数据中并没有出现缺失值，如果相关数据大部分为空值，删去相关的空值数据。

##### 3.2 企业招聘信息
##### 3.2.1 招聘岗位分析
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/8dc04749-a956-4fc7-a8bc-d205e60a09a8)
###### 由于招聘信息中企业发布的工作岗位数据类型较多，本文截取主要的职位进行展示，通过统计的数据可以看出在“大数据+”和“人工智能”领域中，对“数据分析”、“计算机”、“数据产品”、“GIS”类的岗位需求较大，“直播”、“游戏”、“客服”等非计算机类型岗位也有一定需求，求职者可以在该平台上获取适合自身的岗位信息。
##### 3.2.2 学历要求分析
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/74ecb803-ebea-41f1-9c52-6a122b088ae4)
###### 根据爬取的招聘信息统计结果可知，招聘信息学历要求最多的是本科，高达78.0%，17.2%的招聘信息要求学历为大专，发布的招聘信息中对学历没有要求占比为 0.6%，相反，要求硕士学历占比 3.3%，硕士，博士的比例都低于 10%，因此可以说明大多数企业招聘对学历要求是不高的；由上图可以看出，本科和大专的占比是比较大的，企业多为小型或中型企业，从企业招聘现实状况来看，学历要求更多的是本科和大专，博士和硕士相对而言需求较少。
##### 3.2.3 公司成立模式分析
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/1ad2f153-29a5-4401-850c-418d29af439b)
##### 3.2.4 公司规模统计
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/a0684718-6639-4bd7-aaf9-a909b902d6cb)
###### 对获取到的数据 统计 xx 发布数据类岗位企业的规模占比，结果如图 所示。从图可以发现，规模为 50—100 人的企业最多，占比 78.7%，其次是规模为 150 —500 人的企业，占比为 8.4%。规模为 1000—1500 人的企业占比最少，占比 0.1%。说明当前的企业规模主要分布在 50—500 人之间，企业大多数处于发展阶段，只有较少部分企业规模较大。
##### 3.2.5 薪资待遇与工作经验、学历要求
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/f8e39783-0773-496b-8e23-5af8db60f8d6)
###### 根据泰迪内推平台上的招聘信息将工作经验要求划分为以下几种类型：经验不限、1-3 年、3-5 年、5-7 年、5-19 年、7 年以上、10 年。从图中可以看出 5-10年的工作经验对薪资的提升有很大优势，无论是最低薪资待遇还是最高薪资期望待遇，薪资随着经验年限的增加而不断增长，这可能跟互联网 IT 行业发展时间阶段有关，由于互联网行业近十年来发展较快，技术和专业知识不断更新，所以对该行业方面的经验需求还是比较大的。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/9fc7cbab-6036-462f-878e-c3180334be53)
###### 除了上述工作经验会影响薪资，学历要求也是薪资待遇要关注的部分，从图中可以看出在最低薪资待遇里，越高学历的求职者在薪资方面有很大程度的晋升空间，这说明大多数企业对求职者的学历还是比较看重，在高薪资待遇里并非越高学历者薪资越高，据观测，在高薪资待遇里，求职者是已经适应过一段时间的工作培训和工作任务，更多看重的是工作能力和技术而并非学历。
##### 3.2.6 岗位技能分析
###### 对招聘信息中的“岗位技能”做进一步分析统计，细分不同工作岗位名称并加以统计。筛选排除部分没有技能要求的岗位后总共得出有 232 个岗位对技能有要求，标记按工作岗位名称不同计数进行标记，颜色深浅作为工作岗位需要该技能的计数体现，颜色越深代表该技能在岗位的重要性，得出的岗位技能热力图如下图所示。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/9f1c7aaf-c5c0-4575-abd9-485d0e410df4)
###### 由上图可以看出，大多数公司工作岗位对岗位技能要求较多的是“数据开发”能力，其中“项目管理需求分析”能力和“项目管理”能力也占据重要地位，可以说明在数据开发能力和分析能力是当前“大数据”和“人工智能”领域不断向前发展的重要动力。为进一步筛选岗位技能要求，本文以“广东省”为例，对招聘信息中的工作岗位具体地址、岗位技能等字段分别进行统计分析，筛选出所有的大数据职位与其对应 id，按照 id 将职位描述表中相应的大数据职位的岗位描述和任职要求提取出来，然后利用 jieba 对这些文本进行分词，由于文本中有大量的专业术语如：“数据分析”“数据挖掘”“云计算”等，需要添加自定义的用户词典，将这些专业术语添加进去并词频统计绘制出对应数据的词云图，通过颜色对不同词汇作出区分，根据字体大小强调了词汇的出现频率，字体越大的汇在岗位技能要求中被提及的次数最多，同时将词云图设计成省份的形状，暗示了企业需要岗位技能要求分布的区域。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/0098ec2c-4797-4b08-a493-60ddaec6a0eb)
###### 根据图可以看出。“数据”、“数据分析”、“可视化”、“预处理”、“数据挖掘”、“探索”等词语出现频数较大，说明广东省中大多数企业招聘需要的人才需具有良好的数据分析能力，另外，“计算机”、“神经网络”、“测试”等词语也有一定程度的展现。说明招聘岗位对专业知识也有一定要求。
##### 3.2.7 招聘信息画像
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/6a5b8274-acf5-4351-b4ff-e7e57ee8adbb)
###### 通过上述的统计分析描述，招聘信息画像由招聘信息中多维度数据结合而成，主要的画像数据包含公司规模、公司成立模式、企业招聘工作经验要求、学历要求、学历对应岗位的最低薪资和最高薪资待遇、工作经验要求下最低工资和最高工资待遇以及岗位需求量，求职者可通过招聘信息画像匹配符合自身的岗位。

##### 3.3 求职者求职信息
##### 3.3.1 预期岗位
###### 根据爬取到的求职者求职信息进行统计分析，求职者求职信息包含“求职者ID”、“姓名”、“应聘工作岗位”、“性别”、“工作经验”、“最低薪资期望”、“最高薪资期望”、“所在城市”、“上传时间”、“个人技能”等 10项数据。下表为截取的部分数据。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/e0c6278d-99d4-4d26-81db-5a66181d3c5e)
###### 首先对求职信息中的“求职岗位”进行分析，由于统计数据文本中有大量的专业术语如：“数据分析师”“数据挖掘工程师”“算法工程师”等，需要添加自定义词典，将这些专业术语添加进去，然后再进行分词，统计高频词汇，由于不同求职者的求职岗位不同，本次提取出前八位的岗位进行分析，从上表可以看出，“数据分析师”出现频率最高，达到 10844 次，其次是“数据挖掘工程师”，可以看出在“大数据”和“人工智能”领域，求职者岗位意向更多的是关于数据分析和处理方面内容，最后生成的整体预期岗位词云图如图所示。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/0279e1b8-8338-4c55-82c7-c5f53e1f2750)
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/f25b25b5-8101-4da4-9aeb-7f91cd7b8433)
###### 为精确分析求职者的岗位期望，本文依据求职者求职信息中的“性别”字段，分别进行预期岗位的分析。求职信息中获取到的男性数据为 1190 条，女性数据为 975 条。分别使用 collections 库中的 Counter()函数对男性女性数据进行词汇统计，提取出关键词后使用 wordcloud 库绘制对应期望岗位词云图。从图中可以看出男性期望岗位出现频率较高的是“数据分析师”、“数据挖掘工程师”、“Hadoop 大数据开发工程师”等，字体大小强调了词汇的出现频率，字体越大的词汇表示在期望岗位中提及的次数最多。

##### 3.3.2 薪资需求
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/d5ceb103-9c7b-4f36-9c17-875b5cd350ee)
###### 图中可以看出男性薪资期望无论是最高期望还是最低期望，其薪资期望都低于 20000，最高薪资期望比较分散，最低薪资期望分布比较集中，从上下四位数的间距可以看出最低薪资期望分布是平衡的，最高薪资期望则不平衡，最高薪资出现 2 个异常点，最低薪资出现 3 个异常点，但从总体看来最高薪资期望和最低薪资期望两者相差不大，说明男性在薪资期望过程中保持较为平稳的状态。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/ef960238-0acc-4801-94da-aec1d82740ed)
###### 从男性薪资幅度图可以看出最高薪资期望和最低薪资期望之间有一定的距离幅度，最高薪资和最低薪资变化幅度趋势大致一样，两者在 4285—4310 区域之间变化较为平缓，工资越高则两则变化幅度都比较大。侧面说明工资幅度在到达一定程度后会发生较大改动。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/f99c8842-f245-4587-976c-bf1cadcfe80b)
###### 图中可以看出女性薪资期望无论是最高期望还是最低期望，其薪资期望都低于 16000，最高薪资期望比较分散，最低薪资期望分布比较集中，从上下四位数的间距可以看出最低薪资期望分布是平衡的，最高薪资期望则不平衡，女性最高薪资期望出现 1 个异常点，最低薪资出现 2 个异常点，但从总体看来最高薪资期望和最低薪资期望两者相差不大，说明女性在薪资期望过程中保持较为平稳发展的状态。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/256b45d8-ddf8-4feb-be59-8efe80b5d78d)
###### 从女性薪资幅度图可以看出最高薪资期望和最低薪资期望之间有一定的距离幅度，最高薪资和最低薪资变化幅度趋势大致一样，两者在薪资变化上一直都有较大的波动。侧面说明在薪资上可能会出现较大差异。

##### 3.3.3 知识储备
###### 在进行“个人技能”统计情况中，发现“个人技能”一块出现数据空白，因此需要对空白数据值进行清理，删除空值数据 10279 条，剩余男性数据 180 条，女性数据 184 条。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/c1a35899-b747-4f4a-926c-6f0258921a91)
###### 根据求职者求职信息的统计结果可知，拥有“数据预处理”能力的人数最多，“数据探索”和“数据分析”能力位列第二第三。依次为“数据可视化”、“数据建模”、“数据采集”、“神经网络”、“图像处理”、“自然语言处理”。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/3ae79fc2-e5d1-4c9e-9681-f04b945c11df)
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/22775e0f-4e61-40ef-852b-ce63719b4ba2)

##### 3.3.4 用户户籍分布
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/e4b94c32-5159-4826-bf4f-6c2e92c00671)


#### 4、构建岗位匹配度和求职满意度的模型
###### python 内置函数对数据中的‘工作岗位要求经验’特征的相关文字内容进行了整体的整合和添加标签，‘工作岗位要求经验’的标签大致分为'不限', '经验不限', '5-10 年', '1-3 年', '3-5 年', '5-7 年', '7 年以上', 3, 5, 1, 0,10等，不利于我们后续的模型训练，从而我们对'工作岗位要求经验'的标签进行重新定义对‘不限’定义为‘经验不限’等，替换了表格中的 1574 条数据的‘工作岗位要求经验’进行替换标签，使得大部分数据符合大多数人的要求，尽可能最大性去选择，无法做到百分百。
###### 提取‘工作岗位名称、工作岗位最低工资、工作岗位要求经验’和‘应聘工作岗位、最低薪资期望、工作经验、个人技能’两者相合并形成一列新的列名命名为‘xdate、ydate’，为后续的岗位匹配度便于计算，将‘求职者 ID、招聘信息 ID、xdate、ydate’形成一个新表格。
###### 使用 jieba 工具对所得列 xdate、ydate 进行分词，使用自定义的词典，便于我们后续的模型训练，同时新建空列命名为 lable，放置训练结果数值，对所得分词进行词频统计。
###### 在处理出使用的数据后，我们随机提取两个岗位信息做分词匹配测试，如使用 sent1 = "高级大数据开发工程师 Hadoop Hive20000.0 经验不限"、sent2 = " 项目经理产品经理资质管理员电商大数据研究员 60001 年工作经验"作为测试用例，测试结果显示：两个字符串之间的匹配度为"0.36315152049064636"，即该求职者所具有的技能经验和对岗位的要求工资等信息与该岗位需要具备的内容相关性相关性有但不多。
###### 我们采取了多线程的方式对数据进行模型训练，线程是一个轻量级的子进程，是最小的处理单元；是一个单独的执行路径。线程是进程的子集（部分），一个进程可能由多个线程组成。多线程是一种执行模型，它允许多个线程存在于进程的上下文中，以便它们独立执行但共享其进程资源。总而言之，进程是资源分配的最小单位，线程是 CPU 调度的最小单位。一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程。
###### 在该列表情况下并不是每一个求职者固定对应到每个岗位，所以将求职者与每一个岗位的关键字进行匹配训练，取平均值作为该岗位面对求职者的匹配度，在内存内核中无法一次性对两百万条数据进行访问、处理和存储，因此使用数据划分、将每一百个求职者，即对应“157,400”条数据做为一个进程去做存储与分析，共计 16 个进程同时并发去访问数据源，达到快速计算与分流计算的目的。
###### 使用 1574 条岗位数据，10908 个求职者相匹配，是每一个求职者对应每一个岗位数据，选取前 600 个求职者，即“600 * 1574 = 944,400” 大约一百万条数据去做模型匹配度分析，通过 16 个进程获得相应对的匹配度存放 label.xlsx表格文件中，对得到的匹配度进行整理和查询，我们获得了 944400 条匹配度数据，在采用了求职者的要求和它求职那个岗位的要求做文本匹配，算出他们每个人的文本匹配的平均匹配率作为最终的匹配模型训练结果，训练结果主要取决于求职者的要求以及岗位要求是主要的文本匹配，同时匹配度越高代表着岗位与求职者会更加合适，在构建匹配岗位匹配度，是以招聘信息与求职者的要求相匹配，构建求职者满意度则反之。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/247bf811-3199-4d6f-9c48-993e92f5c71e)


#### 5、求职者满意度模型
###### 求职者满意度模型采用相同的数据清洗，是对工资的处理并不相同，我们采用了 result1-2 表格中‘工作岗位最低工资与工作岗位最高工资’的总和求得平均数’作为平均工资，形成后续作为相匹配的要素，resutl1-1 表提取出求职者ID、最高薪资期望、工作经验，使其合成一个表格。通过查询应聘岗位，发现应聘者中想要应聘数据分析师共有 10843 人我们提取了‘平均工资、工作岗位要求经验’和‘最低薪资期望、工作经验’两者相合并形成一列新的列名命名为‘xdate、ydate’，为后续的岗位匹配度便于计算，将‘求职者 ID、招聘信息 ID、xdate、ydate’形成一个新的表格，为后续的文本训练做数据准备。
###### 使用 jieba 工具对所得列 xdate、ydate 进行分词，使用自定义的词典，便于我们后续的模型训练，同时新建空列命名为 label，放置训练结果数值，对所得分词进行词频统计。
###### 在处理出使用的数据后，我们随机提取两个求职者信息做分词匹配测试如使用 sent1 = "12500.01 年工作经验"、sent2 = "15000 无经验"作为测试用例，测试结果显示：两个字符串之间的匹配度为"0.8509632349014282"，即该求职者所具有的技能经验和对岗位的要求工资等信息与该岗位需要具备的内容相关性高。
###### 数据利用 collections 库中的 Counter，对分词的文本使用数值相替换，会更加有利于后续的满意度计算，通过对词频统计对各项词语进行数值转换，转换后的数据表，label 列为空，提前建立一个空列放置计算后的满意度数值，使用的数值为文本分词后的词频统计对文字进行数值替换，对替换下来的数值进行模型训练。
###### 数据匹配计算满意度时，随机生成打乱的索引，然后再组织数据计算，与此循环，计算最终的结果，将具体得出的数据存储到数据表中，训练时采用了二八分原则，训练结果能达到百分之 80 以上，则使用改模型以及数据进行训练；为每位求职者提供求职者满意度非 0 的招聘信息，将结果进行降序排序存放保存在result3-2 表中，部分结果如下图截图图 5-8 满意度训练结果
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/e52c10ac-6b6d-4aa5-a1e2-141ce740e837)

#### 6、招聘求职双向推荐模型
##### 6.1 岗位匹配度的数据处理
###### 由第三问结果得知，企业发布的招聘岗位为 1574 条数据，人才网注册的用户为19908 条数据，根据清洗与建模的数据训练，最终获取到的有效数据为企业发布岗位 1574 条，人才注册实名数据也是 1574 条，但在建模训练数据的过程中，发现对于 1574 个用户对应 1574 个岗位所生成的 2,477,476 条，接近两百五十万条数据在该计算机的运行中不堪重负，即使在后端开设了 16 个进程进行分流和较少内存的运行，也很难准确跑完所有数据，所以从第三题的建模分析开始，就选取了前 700 个注册有效用户进行数据训练，即将近一百万条数据进行训练和分析，第四题也是延续第三题做的分析，因此也是前 700 个有效用户的数据，作为我们双向推荐模型的训练对象。
###### 提取原始表中的 result1-1.csv 中的工作岗位招收人数，与 result3-1.csv 表格数据相合并，将工作岗位招收人数整合到数据表中，获取需要分析的数据集合，形成一个表格数据，为后续的招聘流程提供相关数据。
###### 根据要求对匹配度进行非‘0’，即‘1’的转化筛选，需要对所有的匹配值求平均值，匹配值高于或等于平均匹配度设置为 1，低于平均匹配度设置为 0。首先查看匹配度的平均值为'0.4262311109612694'，之后查看每一个匹配度值均为'float'类型，即数值型，且平均值也是数值型数据，在做 for 遍历不需要转型可以直接运算，转换完查看得知满意度中 0 值有“499504”个数据，满意度中1 值有“444896”个数据。
###### 运算后得出的列表并入原数据中，得知总共有"1"值"4999504"条数据，"0"值"444896"条数据。
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/84333b09-dfb4-48ba-8e50-8a52d3ebeeaf)

##### 6.2 求职者满意度的数据处理
###### 根据要求对满意度进行非‘0’，即‘1’的转化筛选，需要对所有的满意值求平均值，满意值高于或等于平均匹配度设置为 1，低于平均满意度设置为 0，查看满意度的平均值为'0.8494644821586466'，转换完查看得知满意度中 0 值有“121”个数据，满意度中 1 值有“145”个数据。
##### 6.3 offer 的发行以及履约率
######本题赛题描述的招聘流程如下：设某岗位拟聘 n 人，泰迪内推平台向企业推荐岗位匹配度非 0 的 n 位求职者发出第一轮 offer，求职者如果收到多于 1 个岗位的 offer，则求职者选取满意度最高的岗位签约，每个求职者只允许选择 1 个岗位签约。第一轮结束后，平台根据当前各招聘信息的剩余岗位数，向后续被推荐求职者发出第二轮 offer，如此继续，直到招聘人数已满或者向所有拟推荐求职者均已发出 offer 为止。履约率定义为：履约率=所有岗位的签约人数之和/所有拟聘岗位人数之和。
###### 第一轮的 offer 发送，企业端：企业岗位招收人数非 0 的岗位向匹配度值为“1”的人才 id 发 offer，对应满意度也为“1”的求职者设置 offer 一栏的数据为 true，同时企业工作岗位招聘人数减 1，求职者端：由于热门的企业岗位大家都会去投递，而优秀的求职者也有很多企业都发了 offer，但在此途中求职者只能选择最多其中一份岗位去入职，即接受其中一份岗位，且前一位求职者接受的岗位，如果该岗位招收人数没有空缺，那么下一位求职者不能就职这个岗位，只能选择下一个别的岗位，因此，在对第一次企业发了 offer 之后，部分 offer被设置为 true，但仅仅代表这个岗位对符合条件的求职者发了 offer，并不是求职者也确定了 offer，接下来还要去做第二次的求职者的反向选择。
###### 对企业招聘岗位中的招聘人数进行空值处理，查询得出共有 64 个岗位并没有开始招聘，将工作招收人数为 0 的数据设为空值，空值代表着岗位并不需要招聘人才，所有综合考虑我们对空值的相关岗位进行一次性删除空值，得到剩下的843664 条数据，对数据重新设置索引，遍历查询工作表中匹配值为“1”的数据，判断如果求职者表中的满意也为 1，即设置 offer 一栏的数据为“true”。
###### 当求职者查看收到 offer 的求职者总共有 424980 条数据，即为 270 名求职者收到了第一轮的 offer，第一轮 offer 结果如下图图 6-6 第一轮 offer 结果显
![image](https://github.com/Chloe010/Python_Bi-LSTM_Attention/assets/129756356/a85cfa8e-5d3e-4d1d-b9c3-55053c346326)

###### 由于第一轮的履约率只有 0.00012706480304955527 极低，为了提高企业岗位发放 offer 的履约率，进行训练代码的调优，要使得履约率达到最高，但是招聘岗位数量是固定的，只能尽可能减少收到 offer 的人数，即通过准确的发出offer，把 offer 发给意愿更高的求职者。
###### 在发放 offer 的同时先了解求职者对于这个岗位的满意程度，且这个求职者有没有对其他岗位的满意程度也达到了 1，按照对比择优原则，选择“满意程度为 1”的数量尽量少的求职者发放 offer，而不是对所有满足条件的求职者都发放 offer，则是在设置 offer 为 True 之前添加筛选条件，达到双向奔赴最大可能性的训练调优。
###### 调优后的履约率提高了 2.2 倍由于第二轮、第三轮的 offer 发放与第一轮大同小异，数据源本体也没有发生变化，数据的训练结果几近相同










